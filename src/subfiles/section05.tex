\documentclass[main.tex]{subfiles}

\begin{document}
    \justifying

    \section{Performance Analysis}
    \label{section:performance-analysis}

    In this section, we highlight the results from a comprehensive performance testing. We compare the developed CLOUDSC-GT4Py and CLOUDSC2-GT4Py codes against reference Fortran versions and various other programming prototypes. The simulations were performed on three different supercomputers:
    \begin{itemize}
        \item[(i)] Piz Daint\footnote{\url{https://www.cscs.ch/computers/piz-daint}}, an HPE Cray XC40/XC50 system installed at CSCS in Lugano, Switzerland;
        \item[(ii)] MeluXina\footnote{\url{https://docs.lxp.lu/}}, an ATOS BullSequana XH2000 machine hosted by LuxConnect in Bissen, Luxembourg, and procured by the EuroHPC Joint Undertaking (JU) initiative;
        \item[(iii)] the Cray HPE EX235a supercomputer LUMI\footnote{\url{https://docs.lumi-supercomputer.eu/}}, an EuroHPC pre-exascale machine at the Science Information Technology Center (CSC) in Kajaani, Finland.
    \end{itemize}
    On each machine, the CLOUDSC and CLOUDSC2 applications are executed on a single hybrid node, that accommodates one or multiple GPU accelerators alongside the host CPU. An overview of the node architectures for the three considered supercomputers can be found in Table \ref{tab:architecture}. Besides the GT4Py codes, we involve up to four alternative lower-level programming implementations.
    \begin{itemize}
        \item[(a)] The baseline Fortran version, enriched with OpenMP directives for multi-threading execution on CPU.
        \item[(b)] An optimized GPU-enabled version based on OpenACC using the single-column coalesced (SCC) loop layout in combination with loop fusion and temporary local array demotion (so-called ``k-caching''). While the SCC loop layout yields more efficient access to device memory, the k-caching technique reduces the memory footprint by restricting temporary fields on individual horizontal planes.
        \item[(c)] An optimized GPU-enabled version using the source-to-source translation tool Loki.
        \item[(d)] An optimized GPU-enabled version of CLOUDSC including loop fusion and temporary local array demotion. The code is written either in CUDA or HIP, to target both NVIDIA GPUs (shipped with Piz Daint and MeluXina) and AMD GPUs (available on LUMI).
    \end{itemize}

    \begin{table}[t!]
        \setlength\extrarowheight{2pt}
        \centering
        \begin{tabular}{|c|c|c|c|c|}
             \hline
             \textbf{System} & \textbf{CPU} & \textbf{GPU} & \textbf{RAM} & \textbf{NUMA domains} \\
             \hline
             Piz Daint & 1x Intel Xeon E5-2690v3 12c & 1x NVIDIA Tesla P100 16GB & 64 GB & 1 \\
             MeluXina & 2x AMD EPYC Rome 7452 32c & 4x NVIDIA Tesla A100 40GB & 512 GB & 4 \\
             LUMI & 1x AMD EPYC Trento 7A53 64c & 4x AMD Instinct MI250x & 512 GB & 8 \\
             \hline
        \end{tabular}
        \vspace*{0.2cm}
        \caption{Overview of the node architecture for the hybrid partition of Piz Daint, MeluXina and LUMI. Only the technical specifications which are most relevant for the purposes of this paper are reported.}
        \label{tab:architecture}
    \end{table}

    \begin{table}[t!]
        \setlength\extrarowheight{2pt}
        \centering
        \begin{footnotesize}
            \begin{tabular}{|c|c|c|c|}
                 \hline
                 \textbf{Implementation} & \textbf{CLOUDSC} & \textbf{CLOUDSC2: Non-linear} & \textbf{CLOUDSC2: Symmetry test} \\
                 \hhline{|====|}
                 \multicolumn{4}{|c|}{\textbf{Piz Daint}} \\
                 \hline
                 Fortran: OpenMP (CPU) & Intel Fortran 2021.3.0 & Intel Fortran 2021.3.0 & Intel Fortran 2021.3.0 \\
                 Fortran: OpenACC (GPU) & NVIDIA Fortran 21.3-0 & - & - \\
                 Fortran: Loki (GPU) & NVIDIA Fortran 21.3-0 & NVIDIA Fortran 21.3-0 & - \\
                 C: CUDA (GPU) & NVIDIA CUDA 11.2.67 & - & - \\
                 GT4Py: CPU k-first & g++ (GCC) 10.3.0 & g++ (GCC) 10.3.0 & g++ (GCC) 10.3.0 \\
                 GT4Py: DaCe (GPU) & NVIDIA CUDA 11.2.67 & NVIDIA CUDA 11.2.67 & NVIDIA CUDA 11.2.67 \\
                 \hhline{|====|}
                 \multicolumn{4}{|c|}{\textbf{MeluXina}} \\
                 \hline
                 Fortran: OpenMP (CPU) & NVIDIA Fortran 22.7-0 & NVIDIA Fortran 22.7-0 & - \\
                 Fortran: OpenACC (GPU) & NVIDIA Fortran 22.7-0 & - & - \\
                 Fortran: Loki (GPU) & NVIDIA Fortran 22.7-0 & NVIDIA Fortran 22.7-0 & - \\
                 C: CUDA (GPU) & NVIDIA CUDA 11.7.64 & - & - \\
                 GT4Py: CPU k-first & g++ (GCC) 11.3.0 & g++ (GCC) 11.3.0 & g++ (GCC) 11.3.0 \\
                 GT4Py: DaCe (GPU) & NVIDIA CUDA 11.7.64 & NVIDIA CUDA 11.7.64 & NVIDIA CUDA 11.7.64 \\
                 \hhline{|====|}
                 \multicolumn{4}{|c|}{\textbf{LUMI}} \\
                 \hline
                 Fortran: OpenMP (CPU) & ~~ Cray Fortran 14.0.2 ~~ & Cray Fortran 14.0.2 & - \\
                 Fortran: OpenACC (GPU) & Cray Fortran 14.0.2 & - & - \\
                 Fortran: Loki (GPU) & Cray Fortran 14.0.2 & Cray Fortran 14.0.2 & - \\
                 C: HIP (GPU) & - & - & - \\
                 GT4Py: CPU k-first & g++ (GCC) 11.2.0 & g++ (GCC) 11.2.0 & g++ (GCC) 11.2.0 \\
                 GT4Py: DaCe (GPU) & AMD Clang 14.0.0 & AMD Clang 14.0.0 & AMD Clang 14.0.0 \\
                 \hline
            \end{tabular}
        \end{footnotesize}
        \vspace*{0.2cm}
        \caption{For each coding version of the CLOUDSC and CLOUDSC2 dwarfs considered in the performance analysis, the table reports the compiler suite used to compile the codes on Piz Daint, MeluXina and LUMI. The codes are compiled with all major optimization options enabled. Those implementations which are either not available or not working are marked with a dash; more details, as well as a high-level description of each coding implementation, are provided in the text.}
        \label{tab:compiler}
    \end{table}

    \noindent Table \ref{tab:compiler} documents the compiler specifications employed for each of the programming implementations, on Piz Daint, MeluXina and LUMI. We consistently apply the most aggressive optimization, ensuring that the underlying code manipulations do not harm validation. For the different algorithms at consideration, validation is carried out as follows.
    \begin{itemize}
        \item For CLOUDSC and CLOUDSC2NL, the results from each coding version are directly compared with serialized reference data produced on the CPU. For each output field, we perform an element-wise comparison using the NumPy function \pyinline{allclose}\footnote{\url{https://numpy.org/devdocs/reference/generated/numpy.allclose.html}}. Specifically, the GT4Py re-writes validate on both CPU and GPU with an absolute and relative tolerance of $10^{-12}$ and $10^{-18}$ when employing double precision. When reducing the precision to 32-bits, the absolute and relative tolerance levels need to be increased to $10^{-4}$ and $10^{-7}$ on CPU, and $10^{-2}$ and $10^{-7}$ on GPU. In the latter case, we observe that the field representing the enthalpy flux of ice still does not pass validation. We attribute the larger deviation from the baseline data on the device to the different instruction sets underneath CPUs and GPUs.
        \item All implementations of CLOUDSC2TL and CLOUDSC2AD are validated using the Taylor test (cf.\,Algorithm \ref{alg:taylor-test}) and the symmetry test (cf.\,Algorithm \ref{alg:symmetry-test}), respectively. However, the conditions of both tests are not satisfied when using single precision. This is not surprising, since both tests are highly sensitive to round-off errors. Nevertheless, performance numbers for the execution of the algorithms were taken.
    \end{itemize}

    \begin{figure}[t!]
        \centering
        \includegraphics[scale=0.44]{img/performance_daint_2.pdf}
        \caption{Execution time on a single NUMA domain of a hybrid node of the Piz Daint supercomputer for CLOUDSC (left column), CLOUDSC2NL (center column) and the symmetry test for CLOUDSC2TL and CLOUDSC2AD (right column) using either double precision (top row) or single precision (bottom row) floating point arithmetic. Displayed are the multi-threaded Fortran baseline using OpenMP (grey); two GPU-accelerated Fortran implementations, either using OpenACC directives (lime) or the source-to-source translation tool Loki (yellow); an optimized CUDA C version (green); and the GT4Py re-write, either using the GridTools C++ CPU backend with k-first data ordering (blue) or the DaCe GPU backend (orange). All numbers should be intended as an average over $50$ realizations. The panels only show the code versions available and validating at the time of writing.}
        \label{fig:performance-daint}
    \end{figure}


    \noindent To enable a fair comparison between CPU and GPU performance, host codes are executed on all the cores available on a single Non-Uniform Memory Access (NUMA) domain of a compute node, while device codes are launched on the GPU attached to that NUMA domain. In a distributed-memory context, this choice allows to fit the same number of MPI ranks per node, either on CPU or GPU. Table \ref{tab:architecture} reports the number of NUMA partitions per node for Piz Daint, MeluXina and LUMI, with the compute and memory resources being evenly distributed across the NUMA domains. Note that the compute nodes of the GPU partition of LUMI have the low-noise mode activated, which reserves one core per NUMA domain to the operating system, so that only 7 out of 8 cores are available to the jobs. Moreover, each MI250x GPU is split into two virtual GPUs (vGPUs), with each vGPU assigned to a different NUMA domain.

    Figures \ref{fig:performance-daint}-\ref{fig:performance-lumi} visualize the execution times for CLOUDSC (left column), CLOUDSC2NL  (center column) and the symmetry test for CLOUDSC2TL and CLOUDSCAD (right column) for Piz Daint, MeluXina and LUMI, respectively. In each figure, execution times are provided for simulations running either entirely in double precision (FP64; top row) or in single precision (FP32; bottom row). Within each panel, the plotted bars reflect the execution time of the various codes, with a missing bar indicating the corresponding code is either not available or not working properly. Specifically,
    \begin{itemize}
        \item the Fortran version of CLOUDSC2AD can only run on a single OpenMP thread on MeluXina (the issue is still under investigation);
        \item a native GPU-enabled version of CLOUDSC using 32-bit floating point arithmetic does not exist at the time of writing, and no CUDA/HIP implementations are available for CLOUDSC2;
        \item all Fortran-based implementations of the three formulations of CLOUDSC2 can only use double precision computations;
        \item a Loki version of CLOUDSC2TL and CLOUDSC2AD is not available at the time of writing.
    \end{itemize}

    \begin{figure}[t!]
        \centering
        \includegraphics[scale=0.44]{img/performance_mlux_2.pdf}
        \caption{As Fig.\,\ref{fig:performance-daint} but for the MeluXina supercomputer.}
        \label{fig:performance-mlux}
    \end{figure}

    \noindent Notably, the GT4Py re-write of both CLOUDSC and CLOUDSC2 runs on every CPU and GPU architecture included in the study, and can fully employ either double precision or single precision floating point arithmetic. With GT4Py, changing the target architecture (namely, the backend), as well as the precision of computations is as easy as setting a namelist parameter. Particularly, the GT4Py porting of the tangent-linear and adjoint formulations of CLOUDSC2 represents the first ever coding version of such schemes offloading the computationally-intensive stencil kernels to GPU. Nonetheless, we observe that the performance delivered by GT4Py falls short of native implementations both on CPU and GPU. Across all systems and test cases, multi-threaded Fortran is up to three times faster than the GridTools CPU backend of GT4Py using the k-first (C-like) memory layout, while on CLOUDSC the DaCe backend of GT4Py is about $50\,\%$ slower than CUDA/HIP. To rule out the hypothesis that this performance gap can be ascribed to overhead originating from Python, Fig.\,\ref{fig:runtime-fraction} displays the fraction of runtime spent within the stencil code generated by GT4Py leveraging either GridTools or DaCe, and the amount of execution time taken by the Python side of the application (infrastructure and framework code; see Section \ref{section:infrastructure-code}). Across the three supercomputers, the impact of the Python overhead decreases as (i) the complexity and length of computations increase, (ii) the peak throughput and bandwidth delivered by the hardware underneath increase, and (iii) the floating point precision decreases. On average, the Python overhead accounts for $5.4\,\%$ of the total runtime on GPU and $0.4\,\%$ on CPU.

    \begin{figure}[t!]
        \centering
        \includegraphics[scale=0.44]{img/performance_lumi_2.pdf}
        \caption{As Fig.\,\ref{fig:performance-daint} but for the LUMI supercomputer.}
        \label{fig:performance-lumi}
    \end{figure}

     \begin{figure}[t!]
        \centering
        \includegraphics[scale=0.44]{img/runtime_fraction_1.pdf}
        \caption{For the GT4Py re-writes of CLOUDSC (left column), CLOUDSC2NL (center column) and the symmetry test for CLOUDSC2TL and CLOUDSC2AD (right column), fraction of the total execution time spent within the stencil computations (full bars) and the Python side of the application (hatched bars) on Piz Daint, MeluXina and LUMI. Results are shown for the GridTools C++ CPU backend with k-first data ordering (blue) and the DaCe GPU backend (orange), either using double precision (top row) or single precision (bottom row) floating point arithmetic.}
        \label{fig:runtime-fraction}
    \end{figure}

    We observe a significant sensitivity of the GPU performance with respect to the thread block size\footnote{In the Fortran dialect, the thread block size corresponds to the NPROMA.}: for values smaller than 128, performance are degraded across all implementations, with the gap between CUDA/HIP and GT4Py+DaCe being smaller. On the one hand, this signals that even for the DSL approach, some (small) hand-tuning, driven by an in-depth knowledge of the underlying compute architecture, is needed to achieve best performance. On the other, it indicates that there is still room for improvement in the DSL optimization toolchain. In this regard, future endeavors at ECMWF might pick up the code generated by GT4Py and manually iterate over it, in view of its integration into the IFS. We also remark that minimal effort has been invested into performance engineering the application side of both CLOUDSC-GT4Py and CLOUDSC2-GT4Py. This was a deliberate choice, not to affect the readability and modularity of the Python codes. Nonetheless, there exist more n\"aive CUDA C versions of CLOUDSC performing more poorly than GT4Py+DaCe.

    The performance of GT4Py compares more favourably with OpenACC. Although OpenACC is consistently faster on CLOUDSC both on Piz Daint and MeluXina, it is significantly slower on LUMI, especially when using double precision. This could be ascribed to the not-yet-refined support for OpenACC offered by the HPE Cray compiler. In this regard, it should be added that as of now, only the HPE Cray compiler implements GPU offloading capabilities for OpenACC directives on AMD GPUs, meaning that Fortran OpenACC codes require an HPE Cray platform to run on AMD GPUs. On the other hand, GT4Py relies on the HIPCC compiler driver developed by AMD to compile device code for AMD accelerators, and this guarantees a proper functioning irrespective of the machine vendor. It is interesting to note that the DaCe backend of GT4Py executes roughly two-times faster on MeluXina's NVIDIA A100 GPUs, compared to LUMI's AMD Instinct MI250x GPUs. As mentioned above, this is due to the fact that from a software perspective, each physical GPU module on LUMI is considered as two virtual GPUs, so that the code is actually executed on half of a physical GPU card. We can therefore speculate that if using both dies of an AMD Instinct MI250x GPU, performance would be on par with the NVIDIA A100 GPU.

    Finally, we highlight that CLOUDSC-GT4Py and CLOUDSC2-GT4Py are faster than Loki-based implementations on all three machines. This is particularly relevant, since Loki promises the same advantages in terms of portability and productivity as GT4Py.

    \biblio
\end{document}
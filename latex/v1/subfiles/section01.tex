\documentclass[main.tex]{subfiles}

\begin{document}
	\justifying

	\section{Introduction}
    \label{section:introduction}

        Soon after its first public release in 1957, Fortran became the language of choice for weather and climate models \citep{mendez14}. On the one hand, its procedural programming style and built-in support for multi-dimensional arrays has granted Fortran large popularity in the whole scientific computing community. On the other, its low-level nature guarantees fast execution of intensive mathematical operations on vector machines and conventional Central Processing Units (CPUs). In the last decades, these characteristics have permitted to run weather forecasts several times per day under tight operational schedules on High-Performance Computing (HPC) systems \citep{neumann19}.

        In recent years, in response to the simultaneous end of Moore's law and Dennard scaling, and due to the societal challenge to reduce energy consumption, the computer hardware landscape has been undergoing a rapid specialization to prevent unsustainable growth of the power envelope \citep{muller19}. As a result, most supercomputers nowadays have a heterogeneous node design, where energy-efficient accelerators such as Graphics Processing Units (GPUs) co-exist with traditional CPUs. Because Fortran has been conceived with CPU-centric machines in mind, efficient programming of hybrid HPC platforms using the core Fortran language can be challenging \citep{mendez14, lawrence18}. Indeed, the sustained performance of legacy weather and climate model codes written in Fortran has decreased over the decades \citep{schulthess18}, revealing the urgency for algorithmic and software adaptations to remain competitive in the medium and long term \citep{bauer21}.

        Compiler directives (or pragmas) are an attractive solution for parallelization, both to spread a workload across multiple CPU threads, and to offload data and computations to GPU. The most famous incarnations of this programming paradigm are OpenMP \citep{dagum98} and OpenACC \citep{chandrasekaran17}. Because compiler directives accommodate incremental porting and enable non-disruptive software development workflows, they are adopted by many weather and climate modeling groups, who are facing the grand challenge of accelerating large code-bases with thousands of source files and millions of lines of code, which stem from decades of scientific discoveries and software developments \citep{lapillonne17, lapillonne20, randall22}. In order not to threaten the overall readability of the code by exposing low-level instructions, the annotation of Fortran codes with compiler directives can be automated in the pre-processor step of the compilation process using tools such as the CLAW compiler \citep{clement19} or the ECMWF source-to-source translation tool Loki\footnote{\url{github.com/ecmwf-ifs/loki}}. Although pragma-based programming models can support intrusive hardware-specific code transformations, additional specialized optimizations may still be required, which could finally lead to code duplication and worsen maintainability \citep{dahm23}. Moreover, performance and portability are much dependent on the level of support and optimization offered by the compiler stack.

        On the contrary, domain-specific languages (DSLs) separate the code describing the science from the code actually executing on the target hardware, thus enabling \emph{performance-portability}, namely application codes that achieve near-optimal performance on a variety of computer architectures \citep{deakin19}. Large portions of many modeling systems are being rewritten using multiple and diverse DSLs, not necessarily embedded in Fortran. For instance, the dynamical core of the weather prediction model from the COnsortium for Small-scale MOdeling \citep[COSMO;][]{baldauf11} has been rewritten in C++ using the GridTools library \citep{afanasyev21} to port stencil-based operators to GPUs \citep{fuhrer14, fuhrer18}. Similarly, HOMMEXX-NH \citep{bertagna20} is an architecture-portable C++ implementation of the non-hydrostatic dynamical core of the Energy Exascale Earth System model \citep[E3SM;][]{taylor20} harnessing the Kokkos library to express on-node parallelism \citep{edwards14}. The GungHo project for a new dynamical core at the UK Met Office \citep{melvin19, melvin24} blends the LFRic infrastructure with the PSyclone code generator \citep{adams19}. Pace \citep{ben-nun22, dahm23} is a Python rewrite of the Finite-Volume Cubed-Sphere Dynamical Core \citep[FV3;][]{harris13} using GT4Py to accomplish performance-portability and productivity. Similarly, various Swiss partners including MeteoSwiss, ETH Zurich and CSCS are porting the ICOsahedral Non-hydrostatic modeling framework \citep[ICON;][]{zangl15} to GT4Py \citep{luz24}. In another related project \citep{kuhnlein23}, a next-generation model for the IFS at ECMWF is developed in Python with GT4Py building on FVM \citep{smolarkiewicz16, kuehnlein19}.

        The focus of the portability efforts mentioned above is the model dynamical core - the part of the model solving numerically the fundamental nonlinear fluid-dynamics equations. In the present work, we turn the attention to physical parametrizations - which account for the representation of subgrid-scale processes - and additionally address the associated tangent-linear and adjoint algorithms. Parametrizations are being commonly ported to accelerators using OpenACC \citep[e.g., ][]{fuhrer14, yang19, kim21}. Wrappers around low-level legacy physics codes might then be designed to facilitate adoption within higher-level workflows \citep{monteiro18, mcgibbon21}. Lately, first attempts at refactoring physical parametrizations with respect to portability have been documented in the literature. For instance, \citet{watkins23} presented a rewrite of the MPAS-Albany Land Ice (MALI) ice-sheet model using Kokkos. Here, we present a Python implementation of the cloud microphysics schemes CLOUDSC \citep{gt4py-dwarf-p-cloudsc} and CLOUDSC2 \citep{gt4py-dwarf-p-cloudsc2-tl-ad}, which are part of the physics suite of the IFS at ECMWF\footnote{As we mention in Section \ref{section:target-cloud-microphysics-schemes}, the versions of CLOUDSC and CLOUDSC2 considered in this study correspond to older release cycles of the IFS than the one currently used in production.}. Details on the formulation and validation of the schemes are discussed in Section \ref{section:target-cloud-microphysics-schemes}. The proposed Python implementations build upon the GT4Py toolchain, and in the remainder of the paper we use the term CLOUDSC-GT4Py to refer to the GT4Py rewrite of CLOUDSC, while the GT4Py ports of the nonlinear, tangent-linear and adjoint formulations of CLOUDSC2 are collectively referred to as CLOUDSC2-GT4Py. The working principles of the GT4Py framework are illustrated in Section \ref{section:domain-specific-approach-to-scientific-software-development}, where we also advocate the advantages offered by domain-specific software approaches. Section \ref{section:infrastructure-code} sheds some light on the infrastructure code \citep{ifs-physics-common}, and how it can enable composable and reusable model components. In Section \ref{section:performance-analysis}, we compare the performance of CLOUDSC-GT4Py and CLOUDSC2-GT4Py, as measured on three leadership-class GPU-equipped supercomputers, to established implementations in Fortran and C/C++. We conclude the paper with final remarks and future development paths.

    %\biblio
\end{document}
